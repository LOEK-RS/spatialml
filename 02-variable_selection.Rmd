# Variable Selection

## Effect of Additional Variables on Machine Learning Model Extrapolation

```{r setup, include=FALSE}

library(ggplot2)
library(plot3D)


knitr::opts_chunk$set(echo = FALSE, fig.pos = "center")

df1 = data.frame(index = rep(1,10),
                 B1 = seq(10))

df2 = expand.grid(B1 = seq(10),B2 = seq(10))
df2$AOA = ifelse(df2$B1 > 1 & df2$B2 > 1, "No", "Yes")

df3 = data.frame(B1 = c(rep(1, 10), seq(10)), B2 = c(seq(10), rep(1,10)))

df4 = df2
df4$elevation = 1

df4 = rbind(df4, data.frame(B1 = rep(1, 10), B2 = rep(1, 10), AOA = "Yes", elevation = seq(10)))

```


### The basic assumption

Imagine we only have predictors with their values ranging from 1 to 10. Using one predictor to model some response, lets assume we have a sample of 10 points each with a different predictor values. We can assume, that this one predictor gives us a 100 \% Area of Applicability, since we have covered the whole range of predictor space.



```{r, fig.height=1}
ggplot(df1, aes(B1, index))+
    geom_raster(fill = "lightgreen")+
    geom_point()+
    coord_equal()+
    scale_x_continuous(name = "B1", expand = c(0,0), breaks = seq(10))+
    scale_y_continuous(name = NULL, expand = c(0,0))+
    theme(axis.text.y = element_blank(), axis.ticks.y = element_blank())
```


### Extrapolation

If we use a second predictor, also covering its full range from 1 to 10, it is not guaranteed that we cover the whole combined predictor space. The worst case scenario is, that predictor B2 covers is only sampled at a single value of predictor B1.  
This leaves 90\% of the combined predictor space uncovered leading to extrapolations of the machine learning models. We need much more observations (10 values of B2 for each of the 10 values of B1, 100 in total) in order to cover the full predictor space.


<table>
<tr>

<td>
```{r}
ggplot(df2, aes(x = B1, y = B2))+
    geom_raster(aes(fill = AOA))+
    scale_fill_manual(values = c("coral", "lightgreen"))+
    geom_point(data = df3)+
    coord_equal()+
    scale_x_continuous(expand = c(0,0), breaks = seq(10))+
    scale_y_continuous(expand = c(0,0), breaks = seq(10))
```
</td>

<td>
```{r}
ggplot(df2, aes(x = B1, y = B2))+
    geom_raster(fill = "lightgreen")+
    geom_point(data = df2)+
    coord_equal()+
    scale_x_continuous(expand = c(0,0), breaks = seq(10))+
    scale_y_continuous(expand = c(0,0), breaks = seq(10))
```
</td>
</tr>
</table>



### One more variable

This effect is magnified in higher dimensions. With just 3 of our fictional variables, we would need 10x10x10 = 1000 sampling points to cover the whole predictor space.
Preventing extrapolation is therefore much more likely with fewer variables, since we have a much higher chance to cover the full combined predictor space.

```{r}
plot3D::points3D(y = df4$B1, z = df4$B2, x = df4$elevation, col = "black",
                 xlab = "Elevation", ylab = "B1", zlab = "B2", 
                 bty = "g")
```


